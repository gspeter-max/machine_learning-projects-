{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1️⃣ Load data (Parquet/CSV).\n",
    "2️⃣ Feature Engineering (convert media_type to numerical, extract day/hour from timestamp).\n",
    "3️⃣ Assemble features using VectorAssembler.\n",
    "4️⃣ Train a Random Forest model (regression or classification).\n",
    "5️⃣ Evaluate model performance (RMSE for regression, Accuracy/AUC for classification).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/workspaces/machine_learning-projects-/pending_work/pyspark task/small_media_data.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  SparkSession \n\u001b[1;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate() \n\u001b[0;32m----> 5\u001b[0m df_csv \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/workspaces/machine_learning-projects-/pending_work/pyspark task/small_media_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m num_partitions \u001b[38;5;241m=\u001b[39m df_csv\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mgetNumPartitions()  \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(num_partitions) \n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/workspaces/machine_learning-projects-/pending_work/pyspark task/small_media_data.csv."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import  SparkSession \n",
    "\n",
    "spark = SparkSession.builder.appName('pipeline').getOrCreate() \n",
    "\n",
    "df_csv = spark.read.csv('/workspaces/machine_learning-projects-/pending_work/pyspark task/small_media_data.csv',\n",
    "                        header = True, inferSchema = True)\n",
    "\n",
    "num_partitions = df_csv.rdd.getNumPartitions()  \n",
    "print(num_partitions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_csv = df_csv.repartition(num_partitions + 10)\n",
    "df_csv.write.option('compression','snappy').mode('overwrite').parquet('/workspaces/machine_learning-projects-/pending_work/pyspark task/small_media_data.parquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----+------+--------+------------------+------+----------------+\n",
      "|post_id|user_id|      date|likes|shares|comments|media_type_encoded|months|engagement_score|\n",
      "+-------+-------+----------+-----+------+--------+------------------+------+----------------+\n",
      "|    371|    126|2025-03-12|  516|   473|       5|                 1|     3|               1|\n",
      "|    997|     34|2025-03-05|  259|   266|     208|                 2|     3|               0|\n",
      "|    823|    105|2025-03-02|  462|    62|     241|                 1|     3|               0|\n",
      "|    771|    103|2025-03-22|  845|     1|     274|                 3|     3|               1|\n",
      "|    594|     17|2025-03-23|  212|   156|     182|                 3|     3|               0|\n",
      "|    133|      8|2025-03-13|  123|   313|     182|                 3|     3|               0|\n",
      "|    321|     47|2025-03-12|  811|    20|     132|                 2|     3|               1|\n",
      "|    928|     57|2025-03-04|  771|   277|     250|                 3|     3|               1|\n",
      "|    529|     92|2025-03-06|  982|   263|     178|                 2|     3|               1|\n",
      "|     29|     57|2025-03-26|  601|   244|      81|                 3|     3|               1|\n",
      "|    514|     62|2025-03-01|  286|   375|     115|                 3|     3|               0|\n",
      "|    757|    122|2025-03-09|  287|   265|      62|                 2|     3|               0|\n",
      "|    237|    190|2025-02-25|  438|    65|      64|                 1|     2|               0|\n",
      "|      2|     51|2025-03-21|  426|   398|     245|                 3|     3|               0|\n",
      "|    735|     54|2025-03-08|  616|   146|     144|                 3|     3|               1|\n",
      "|     73|     63|2025-03-14|  710|   413|      36|                 2|     3|               1|\n",
      "|     67|     84|2025-02-27|  454|    30|      29|                 1|     2|               0|\n",
      "|    659|     15|2025-03-19|  954|   294|     127|                 1|     3|               1|\n",
      "|    484|    123|2025-03-05|  916|   386|     187|                 3|     3|               1|\n",
      "|    837|    159|2025-02-26|  352|   439|      60|                 2|     2|               0|\n",
      "+-------+-------+----------+-----+------+--------+------------------+------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet('/workspaces/machine_learning-projects-/pending_work/pyspark task/small_media_data.parquet')\n",
    "\n",
    "\n",
    "'''\n",
    "# if you do that in sql \n",
    "\n",
    "df_parquet.createOrReplaceTempView('media_table')\n",
    "df_parquet = spark.sql(\n",
    "    \"\"\"\n",
    "        with temp_temp as (\n",
    "            select \n",
    "                post_id, \n",
    "                user_id, \n",
    "                cast(timestamp as date) as dates, \n",
    "                cast(date_format(timestamp,\"M\") as int)  as months , \n",
    "                likes, \n",
    "                case when likes > 500 then 1 else 0 end as engagement_score, \n",
    "                case when media_type = 'image' then 1 \n",
    "                    when media_type = 'text' then 2 \n",
    "                    else 3 end as media_type_encoded\n",
    "\n",
    "            from media_table \n",
    "        )\n",
    "        select * from temp_temp;\n",
    "    \"\"\"\n",
    ")\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import when,cast , col , date_format,cast \n",
    "from pyspark.sql.types  import DateType  \n",
    "\n",
    "df_parquet = df_parquet.withColumn('timestamp' , col('timestamp').cast(DateType())).withColumnRenamed('timestamp','date')\n",
    "df_parquet = df_parquet.withColumn('months' , date_format('date', 'M').cast('int'))\n",
    "df_parquet= df_parquet.withColumn('engagement_score', when(col('likes') > 500 ,1).otherwise(0))\n",
    "df_parquet = df_parquet.withColumn('media_type',when(col('media_type') == 'image', 1).when(col('media_type') == 'video',2).otherwise(3)).withColumnRenamed('media_type', 'media_type_encoded')\n",
    "df_parquet.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train , test = df_parquet.randomSplit([0.8,0.2], seed= 42)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "\n",
    "train = train.drop('post_id','user_id','date')\n",
    "\n",
    "column_list = train.columns\n",
    "column_list.remove('engagement_score')\n",
    "\n",
    "vector = VectorAssembler(\n",
    "    inputCols = column_list,\n",
    "    outputCol = 'vector_list'\n",
    ")\n",
    "train_transformed = vector.transform(train)\n",
    "test_transformed = vector.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestClassifier(\n",
    "    featuresCol = 'vector_list', \n",
    "    labelCol = 'engagement_score'\n",
    ")\n",
    "model = model.fit(train_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----+------+--------+------------------+------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "|post_id|user_id|      date|likes|shares|comments|media_type_encoded|months|engagement_score|         vector_list|       rawPrediction|         probability|prediction|\n",
      "+-------+-------+----------+-----+------+--------+------------------+------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "|      3|     29|2025-03-23|  969|   197|     148|                 3|     3|               1|[969.0,197.0,148....|[0.03698201760166...|[0.00184910088008...|       1.0|\n",
      "|      8|      1|2025-03-14|  537|   358|      45|                 2|     3|               1|[537.0,358.0,45.0...|[0.55787569833490...|[0.02789378491674...|       1.0|\n",
      "|     12|     90|2025-03-10|  880|   324|     146|                 3|     3|               1|[880.0,324.0,146....|[0.04165491479792...|[0.00208274573989...|       1.0|\n",
      "|     29|     57|2025-03-26|  601|   244|      81|                 3|     3|               1|[601.0,244.0,81.0...|[0.02477470749315...|[0.00123873537465...|       1.0|\n",
      "|     40|    135|2025-03-07|  687|   467|     293|                 3|     3|               1|[687.0,467.0,293....|[0.04601653755851...|[0.00230082687792...|       1.0|\n",
      "|     47|    108|2025-03-11|  970|    69|     213|                 2|     3|               1|[970.0,69.0,213.0...|[0.01220731010850...|[6.10365505425471...|       1.0|\n",
      "|     60|    150|2025-03-16|  278|   296|      56|                 2|     3|               0|[278.0,296.0,56.0...|[19.5284280936454...|[0.97642140468227...|       0.0|\n",
      "|     71|    118|2025-03-10|   64|    35|     291|                 3|     3|               0|[64.0,35.0,291.0,...|          [20.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|     91|    168|2025-03-23|  621|   105|     289|                 3|     3|               1|[621.0,105.0,289....|[0.03212764866962...|[0.00160638243348...|       1.0|\n",
      "|     96|    115|2025-03-16|  776|   328|     187|                 2|     3|               1|[776.0,328.0,187....|[0.04165491479792...|[0.00208274573989...|       1.0|\n",
      "|     98|    194|2025-03-25|   84|   227|     289|                 2|     3|               0|[84.0,227.0,289.0...|          [20.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|    100|    114|2025-03-20|  327|   106|      60|                 1|     3|               0|[327.0,106.0,60.0...|[19.5284280936454...|[0.97642140468227...|       0.0|\n",
      "|    102|    179|2025-03-20|  449|   337|     122|                 1|     3|               0|[449.0,337.0,122....|[19.5284280936454...|[0.97642140468227...|       0.0|\n",
      "|    108|     55|2025-02-28|  664|   177|     143|                 3|     2|               1|[664.0,177.0,143....|[0.03698201760166...|[0.00184910088008...|       1.0|\n",
      "|    120|    135|2025-02-26|  342|   471|     220|                 3|     2|               0|[342.0,471.0,220....|          [20.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|    133|      8|2025-03-13|  123|   313|     182|                 3|     3|               0|[123.0,313.0,182....|          [20.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|    143|    196|2025-03-12|  469|   402|     177|                 1|     3|               0|[469.0,402.0,177....|          [19.0,1.0]|         [0.95,0.05]|       0.0|\n",
      "|    176|    166|2025-03-21|  908|   197|     146|                 1|     3|               1|[908.0,197.0,146....|[0.55805717007067...|[0.02790285850353...|       1.0|\n",
      "|    193|     40|2025-03-11|  171|   166|     212|                 3|     3|               0|[171.0,166.0,212....|          [20.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|    202|    109|2025-03-12|  117|   137|     200|                 1|     3|               0|[117.0,137.0,200....|          [20.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "+-------+-------+----------+-----+------+--------+------------------+------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "roc : 0.9950062421972535\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import  BinaryClassificationEvaluator\n",
    "\n",
    "prediction = model.transform(test_transformed)\n",
    "prediction.show()\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol = 'engagement_score',\n",
    "    rawPredictionCol = 'probability', \n",
    "    metricName = 'areaUnderROC'\n",
    ")\n",
    "roc = evaluator.evaluate(prediction)\n",
    "print(f'roc : {roc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
